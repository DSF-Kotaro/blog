{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "図解速習DeepLearning_015.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQdP6BRscXXhgNxZsgRAOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DSF-Kotaro/blog/blob/main/%E5%9B%B3%E8%A7%A3%E9%80%9F%E7%BF%92DeepLearning_015.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CAy0ZSVkKOZ"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsE-mI18kXX2",
        "outputId": "773c8241-3bc3-4e52-c1b5-59f18af2e2a9"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZrLhN0dkkXN",
        "outputId": "ac2d6f85-f5da-4d54-f309-d9fb136a08bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
        "\r\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEjxqPYT0Xg7",
        "outputId": "208ac638-5838-4732-9fe0-b85b71ff6af0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbhaLKuC0ZPF",
        "outputId": "9829526e-ad16-4519-8b48-f7860b7cc7ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = sorted(set(text))\r\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFI3MVyQ0bgH"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\r\n",
        "idx2char = np.array(vocab)\r\n",
        "\r\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHAISrH50eMi",
        "outputId": "db345b20-1f46-4b22-a126-8c7e3bfa820c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('{')\r\n",
        "for char,_ in zip(char2idx, range(20)):\r\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\r\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmDALFPt0gE0",
        "outputId": "a82d9220-8e0b-4097-dc0b-6daae30dd107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUHZBs4N0jYK",
        "outputId": "aca8db53-efd9-4a28-8c80-61e3b8de34f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seq_length = 100\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "\r\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n",
        "\r\n",
        "for i in char_dataset.take(5):\r\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUOx0EnS0m8A",
        "outputId": "f111a331-b12c-45bc-bdc9-fa17d64da4c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\r\n",
        "\r\n",
        "for item in sequences.take(5):\r\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhBkuTei0pGI"
      },
      "source": [
        "def split_input_target(chunk):\r\n",
        "    input_text = chunk[:-1]\r\n",
        "    target_text = chunk[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VndXMU6u0tUI",
        "outputId": "a3ee6d7e-ad5c-4f3e-84fc-2afbb61ad1d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\r\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\r\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxh4tRPm01mf",
        "outputId": "c465c9fc-6039-4a07-f6db-9d60c929b8d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\r\n",
        "    print(\"Step {:4d}\".format(i))\r\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\r\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PoE0hcj03dX",
        "outputId": "7959944d-d039-4507-e186-9bdef27a7fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "\r\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOpR7kyo05pN"
      },
      "source": [
        "vocab_size = len(vocab)\r\n",
        "\r\n",
        "embedding_dim = 256\r\n",
        "\r\n",
        "rnn_units = 1024"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-apNOGE07f3"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "  model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n",
        "                              batch_input_shape=[batch_size, None]),\r\n",
        "    tf.keras.layers.GRU(rnn_units,\r\n",
        "                        return_sequences=True,\r\n",
        "                        stateful=True,\r\n",
        "                        recurrent_initializer='glorot_uniform'),\r\n",
        "    tf.keras.layers.Dense(vocab_size)\r\n",
        "  ])\r\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uViKN6pT09Cx"
      },
      "source": [
        "model = build_model(\r\n",
        "  vocab_size = len(vocab),\r\n",
        "  embedding_dim=embedding_dim,\r\n",
        "  rnn_units=rnn_units,\r\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InjDuuQM0-4m",
        "outputId": "ed3c2d2e-9ba1-4bf1-cceb-eae61ba4c166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "  example_batch_predictions = model(input_example_batch)\r\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ0931DS1B_s",
        "outputId": "8b1809eb-b5a8-4410-b150-e768472167eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0CVlE7_1EFS"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDFxr-jh1GOE",
        "outputId": "6910c332-e1a8-455b-b849-a3632a71218b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2, 56, 30,  8, 61,  6, 63, 57,  9, 26, 29, 12, 29, 63, 63, 34, 27,\n",
              "       23, 24,  7, 13, 29, 42, 43, 38,  7, 63, 30, 25, 61, 28,  3, 53, 35,\n",
              "       25,  7, 42, 31, 31, 11,  1,  0, 43, 45,  6, 15, 10, 21, 29,  5, 60,\n",
              "       20, 59, 35, 18, 34, 28, 62, 27, 33, 21, 55, 58, 22, 38, 48, 35, 24,\n",
              "       51,  7, 30, 13, 57, 25, 39, 60, 42, 20, 40, 46,  8, 18, 53, 43, 64,\n",
              "       13, 54, 23,  8, 47, 42,  1, 45, 37, 63, 43, 30, 44, 15, 44])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJGT0EjL1Hqm",
        "outputId": "84707ef6-ea28-4995-a79b-32f7eaef4d82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\r\n",
        "print()\r\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \" say true: as Hereford's love, so his;\\nAs theirs, so mine; and all be as it is.\\n\\nNORTHUMBERLAND:\\nMy \"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"!rR.w,ys3NQ?QyyVOKL-AQdeZ-yRMwP$oWM-dSS; \\neg,C:IQ'vHuWFVPxOUIqtJZjWLm-RAsMavdHbh.FoezApK.id gYyeRfCf\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my1i8N7l1JQj",
        "outputId": "4a3b118d-1e38-446d-c80f-095bf969bb40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def loss(labels, logits):\r\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n",
        "\r\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\r\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.174324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EHQj7Py1LNn"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_jNPRlz1MpU"
      },
      "source": [
        "# チェックポイントが保存されるディレクトリ\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "# チェックポイントファイルの名称\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbs0TV171ON8"
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EBJpnKe1Qp6",
        "outputId": "7422f244-e777-45ee-9c32-049295bd47d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 3.2097\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 2.0411\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.7323\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.5603\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.4657\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3963\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3453\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3037\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2657\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uq_6Y8G1SZF",
        "outputId": "f1311dc7-c119-4bfa-a5d5-b2a15acaa266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xayWotuA1U_Z"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n",
        "\r\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
        "\r\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J72TaFSH1WpS",
        "outputId": "4c28e443-f501-4267-b64e-20407094c6e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpcWUGHt1YGa"
      },
      "source": [
        "def generate_text(model, start_string):\r\n",
        "\r\n",
        "  num_generate = 1000\r\n",
        "\r\n",
        "  input_eval = [char2idx[s] for s in start_string]\r\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "\r\n",
        "  text_generated = []\r\n",
        "\r\n",
        "  temperature = 1.0\r\n",
        "\r\n",
        "  model.reset_states()\r\n",
        "  for i in range(num_generate):\r\n",
        "      predictions = model(input_eval)\r\n",
        "      predictions = tf.squeeze(predictions, 0)\r\n",
        "\r\n",
        "      predictions = predictions / temperature\r\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
        "\r\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "      text_generated.append(idx2char[predicted_id])\r\n",
        "\r\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T63u5FW41ay1",
        "outputId": "48617c3c-bdc6-48e2-a7e6-1462dcf7539e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: Nurse!\n",
            "Nay, get thee, or if you?\n",
            "Had not he hath an oath but nobly cengman's haspeth war\n",
            "that thou wilt have him baxhaline.\n",
            "Now, sir, by Than's eyes fastel old,\n",
            "That cannot be forget i' the suitor,\n",
            "Northumberling\n",
            "speaks thou hadst kepore thine eyes;\n",
            "No, nothing budgen'd of my forseman.\n",
            "\n",
            "HERMIONE:\n",
            "Most he to be heell ying, sir.\n",
            "\n",
            "VINCENTIO:\n",
            "\n",
            "TRANIO:\n",
            "Nay, hear you so? O wood and truth!\n",
            "O, who bloody Captifts yourself, which have done low out.\n",
            "\n",
            "TRANIO:\n",
            "Nay, asains her doth come to gie, and strike\n",
            "My fian I' the friendly father?\n",
            "\n",
            "LUCENTIO:\n",
            "A grace!\n",
            "\n",
            "HERMIONE:\n",
            "Northumberland horse and my sin pe\n",
            "dispersedomer, and therein time and me:\n",
            "If they say 'I'll keep. Death, wisdom.\n",
            "March to thy knift and courtesy, as stands?\n",
            "Are you not, take that all the damn\n",
            "To this wry 'twixt so scould have done'er strike rests,\n",
            "Sevately hours: no dreddicinn have\n",
            "with her, though by his tadster kind by many. Yes, keep your heads,\n",
            "That look'd on and stand, what an inflicted of\n",
            "himself, for we are.\n",
            "Let not how st fea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoem87L-1cnQ",
        "outputId": "32959ca3-fcdc-42b3-8cf5-5a011d00c1d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_model(\r\n",
        "  vocab_size = len(vocab),\r\n",
        "  embedding_dim=embedding_dim,\r\n",
        "  rnn_units=rnn_units,\r\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V60txjTe1f0I"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtY6ddnY1hS1"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(inp, target):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    predictions = model(inp)\r\n",
        "    loss = tf.reduce_mean(\r\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\r\n",
        "            target, predictions, from_logits=True))\r\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\r\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
        "\r\n",
        "  return loss"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe_92jws1i57",
        "outputId": "89f8dbc2-1995-4435-98dc-dadcfb15ef4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  hidden = model.reset_states()\r\n",
        "\r\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\r\n",
        "    loss = train_step(inp, target)\r\n",
        "\r\n",
        "    if batch_n % 100 == 0:\r\n",
        "      template = 'Epoch {} Batch {} Loss {}'\r\n",
        "      print(template.format(epoch+1, batch_n, loss))\r\n",
        "\r\n",
        "  if (epoch + 1) % 5 == 0:\r\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\r\n",
        "\r\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\r\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\r\n",
        "\r\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.173957824707031\n",
            "Epoch 1 Batch 100 Loss 2.3314778804779053\n",
            "Epoch 1 Loss 2.1555\n",
            "Time taken for 1 epoch 11.281167268753052 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1367037296295166\n",
            "Epoch 2 Batch 100 Loss 1.9241223335266113\n",
            "Epoch 2 Loss 1.8167\n",
            "Time taken for 1 epoch 10.425826072692871 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7992395162582397\n",
            "Epoch 3 Batch 100 Loss 1.6966116428375244\n",
            "Epoch 3 Loss 1.5534\n",
            "Time taken for 1 epoch 10.46082091331482 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5998129844665527\n",
            "Epoch 4 Batch 100 Loss 1.534644603729248\n",
            "Epoch 4 Loss 1.5422\n",
            "Time taken for 1 epoch 10.56141471862793 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4495196342468262\n",
            "Epoch 5 Batch 100 Loss 1.4283349514007568\n",
            "Epoch 5 Loss 1.4395\n",
            "Time taken for 1 epoch 10.753752946853638 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.37766432762146\n",
            "Epoch 6 Batch 100 Loss 1.4204047918319702\n",
            "Epoch 6 Loss 1.4169\n",
            "Time taken for 1 epoch 10.712035655975342 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3377376794815063\n",
            "Epoch 7 Batch 100 Loss 1.3669263124465942\n",
            "Epoch 7 Loss 1.3799\n",
            "Time taken for 1 epoch 10.611849069595337 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2834670543670654\n",
            "Epoch 8 Batch 100 Loss 1.357453465461731\n",
            "Epoch 8 Loss 1.3340\n",
            "Time taken for 1 epoch 10.556845426559448 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2237904071807861\n",
            "Epoch 9 Batch 100 Loss 1.2785512208938599\n",
            "Epoch 9 Loss 1.2739\n",
            "Time taken for 1 epoch 10.519307136535645 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.195039987564087\n",
            "Epoch 10 Batch 100 Loss 1.2614940404891968\n",
            "Epoch 10 Loss 1.2562\n",
            "Time taken for 1 epoch 10.561087369918823 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}